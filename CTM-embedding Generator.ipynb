{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6a908d",
   "metadata": {},
   "source": [
    "# CTM-EMbeddings\n",
    "The below code comprises both the CTM algorithm itself as well as the fetching and extraction of necessary data from the (arXiv.org) API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2c49ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: conda: command not found\n",
      "[5 5 5 5 5]\n",
      "/bin/bash: conda: command not found\n",
      "/bin/bash: conda: command not found\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ym/47xk9jjx7_qb9zbpnylll5nm0000gn/T/ipykernel_10691/1222343902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda install --yes --prefix {sys.prefix} requests as request'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda install --yes --prefix {sys.prefix} pandas as pd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conda install --yes --prefix {sys.prefix} scipy.optimise'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mminimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "e= \"\"\"\n",
    "import numpy as np\n",
    "import requests as request\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "\"\"\"\n",
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} numpy as np\n",
    "print(np.full(5,5))\n",
    "!conda install --yes --prefix {sys.prefix} requests as request\n",
    "!conda install --yes --prefix {sys.prefix} pandas as pd\n",
    "pd.DataFrame([1,2],[1,2])\n",
    "!conda install --yes --prefix {sys.prefix} scipy.optimise\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba70c9",
   "metadata": {},
   "source": [
    "## Correlated Topic Model Algorithm\n",
    "First: some global variables where K is the number of topics\n",
    "\n",
    "Second:\n",
    "1. Evidence Lower Bound - ELBO computation\n",
    "2. Optimisation of document scope local variable Zeta that is part of the additional variational inference approximation within the variational inference for the latent variables \n",
    "3. Optimisation of Phi, the free parameter of the variational density for topic-assignment variable z\n",
    "4. Optimisation of Lambda, the mean of the variational density for latent document-topic variable Eta\n",
    "5. Optimisation of Nu, the variance of the variational density \n",
    "6. Random initialisation of the free parameters of the variational densities\n",
    "7. Random initialisation of the latent global model parameters \n",
    "8. Local optimisation of one document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = []  # documents in words\n",
    "corpus = []  # documents in IDs\n",
    "words = {}  # word : ID\n",
    "words_inv = {}  # ID : word\n",
    "K = 25  # number of topics\n",
    "\n",
    "\n",
    "def ELBO(latent_param, free_param, doc):\n",
    "    mu, sigma, beta = latent_param\n",
    "    zeta, phi, nu, lam = free_param\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    lam_mu_diff = (lam-mu)\n",
    "    term_1 = 0.5*np.log(np.linalg.det(sigma_inv))-(K/2)*np.log(2*np.pi)\\\n",
    "             -(0.5*np.trace(np.dot(np.diag(nu), sigma_inv))\n",
    "               +np.dot(np.dot(np.transpose(lam_mu_diff), sigma_inv), lam_mu_diff))\n",
    "    term_2 = 0\n",
    "    term_2_1 = -(1/zeta)*sum(np.exp(lam+nu/2))+1-np.log(zeta)\n",
    "    term_3 = 0\n",
    "    term_4 = 0.5*sum(np.log(nu)+np.log(2*np.pi)+1)\n",
    "    term_4_1 = 0\n",
    "\n",
    "    for n, word in enumerate(doc):\n",
    "        term_2 += np.dot(lam, phi[n])\n",
    "        term_3 += np.dot(phi[n], np.log(beta[:, int(word)]))\n",
    "        term_4_1 += np.dot(phi[n], np.log(phi[n]))\n",
    "\n",
    "    term_2 += len(doc)*term_2_1\n",
    "    term_4 -= term_4_1\n",
    "    res = term_1 + term_2 + term_3 + term_4\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def opt_zeta(free_param):\n",
    "\n",
    "    zeta, phi, nu, lam = free_param\n",
    "    zeta = sum(np.exp(lam+nu/2))\n",
    "\n",
    "    return zeta\n",
    "\n",
    "\n",
    "def opt_phi(latent_param, free_param, doc):\n",
    "    mu, sigma, beta = latent_param\n",
    "    zeta, phi, nu, lam = free_param\n",
    "\n",
    "    phi = np.ones((len(doc), K))\n",
    "    for n, word in enumerate(doc):\n",
    "        summation = 0\n",
    "        for i in range(K):\n",
    "\n",
    "            phi[n, i] = np.exp(lam[i])*beta[i, int(word)]\n",
    "            summation += phi[n, i]\n",
    "        phi[n] = phi[n]/summation\n",
    "    return phi\n",
    "\n",
    "\n",
    "def opt_lam(latent_param, free_param, doc):\n",
    "    mu, sigma, beta = latent_param\n",
    "    zeta, phi, nu, lam = free_param\n",
    "\n",
    "    def elbo_df_lam(x):\n",
    "        return np.dot(np.linalg.inv(sigma), (x-mu))-sum(phi)+(len(doc)/zeta)*np.exp(x+nu/2)\n",
    "\n",
    "    def elbo_lam(x):\n",
    "        return -ELBO(latent_param, (zeta, phi, nu, x), doc)\n",
    "\n",
    "    optimized_lambda = minimize(fun=elbo_lam, x0=lam, jac=elbo_df_lam, method='Newton-CG', options={'disp': 0,'xtol': 0.00001})\n",
    "\n",
    "    return optimized_lambda.x\n",
    "\n",
    "\n",
    "def opt_nu(latent_param, free_param, doc):\n",
    "    mu, sigma, beta = latent_param\n",
    "    zeta, phi, nu, lam = free_param\n",
    "    N = len(doc)\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    df_1 = lambda x, i, s_inv, l: -0.5*s_inv-0.5*(N/zeta)*np.exp(l + x/2)+1/(2*x)\n",
    "    df_2 = lambda x, i, l: -0.25*(N/zeta)*np.exp(l+x/2)-0.5*(1/(x*x))\n",
    "\n",
    "    for i in range(K):\n",
    "        s_inv = sigma_inv[i, i]\n",
    "        l = lam[i]\n",
    "        x = 5\n",
    "        log_x = np.log(x)\n",
    "        df1 = 1\n",
    "        x_safety = 10\n",
    "        while np.abs(df1) > 0.0001:\n",
    "            if np.isnan(x):\n",
    "                x_safety *= 2  # Two was used in the original C-code provided by the paper itself\n",
    "                x = x_safety\n",
    "                log_x = np.log(x)\n",
    "                print(\"nan detected, value set to \", x)\n",
    "\n",
    "            x = np.exp(log_x)\n",
    "            df1 = df_1(x, i, s_inv, l)\n",
    "            df2 = df_2(x, i, l)\n",
    "            log_x -= (x*df1) / (x*x*df2+x*df1)\n",
    "        nu[i] = np.exp(log_x)\n",
    "    return nu\n",
    "\n",
    "\n",
    "def random_free_param(doc):\n",
    "    zeta = 2 #Zeta set to Two as was done in the original paper Blei and Lafferty (2006)\n",
    "    phi = np.random.dirichlet(np.ones(K), len(doc))\n",
    "    nu = np.ones(K)\n",
    "    lam = np.zeros(K)\n",
    "    return zeta, phi, nu, lam\n",
    "\n",
    "\n",
    "def init_latent_param(V):\n",
    "    mu = np.zeros(K)\n",
    "    sigma = np.eye(K)\n",
    "    beta = np.random.dirichlet(np.ones(V), K)\n",
    "    return mu, sigma, beta\n",
    "\n",
    "\n",
    "def opt_doc(latent_param, doc, free_param):\n",
    "\n",
    "    zeta, phi, nu, lam = free_param\n",
    "    bound_old = ELBO(latent_param, free_param, doc)\n",
    "    iters = 0\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "        zeta = opt_zeta(free_param)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        lam = opt_lam(latent_param, free_param, doc)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        zeta = opt_zeta(free_param)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        nu = opt_nu(latent_param, free_param, doc)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        zeta = opt_zeta(free_param)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        phi = opt_phi(latent_param, free_param, doc)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "\n",
    "        bound = ELBO(latent_param, free_param, doc)\n",
    "\n",
    "        if np.abs((bound_old-bound)/bound_old) > 0.00001 and iters < 500:\n",
    "            bound_old = bound\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return zeta, phi, nu, lam, bound_old\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89acba",
   "metadata": {},
   "source": [
    "## Expectation Step\n",
    "Here, all documents are optimised w.r.t ELBO and passed to the Maximisation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ebe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expectation_step(corpus, old):\n",
    "\n",
    "    corpus_free_params = []\n",
    "    elbo_sum = 0\n",
    "    for i, doc in enumerate(corpus):\n",
    "        if old == 0:\n",
    "            free_param = random_free_param(doc)\n",
    "        else:\n",
    "            zeta = 2\n",
    "            phi = old[i][0]\n",
    "            nu = old[i][1]\n",
    "            lam = old[i][2]\n",
    "            free_param = zeta, phi, nu, lam\n",
    "        values = opt_doc(latent_param, doc, free_param=free_param)\n",
    "        zeta, phi, nu, lam, bound_old = values\n",
    "        corpus_free_params.append((phi.copy(), nu.copy(), lam.copy()))\n",
    "        elbo_sum += bound_old\n",
    "    return corpus_free_params, elbo_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c337f2",
   "metadata": {},
   "source": [
    "## Maximisation Step\n",
    "Here, the global scoped latent model parameters (Mu, Sigma, Beta) are optimised w.r.t ELBO given all the document-scope free  parameters from the optimised documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79720f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization_step(corpus_free_params):\n",
    "\n",
    "    mu, sigma, beta = latent_param\n",
    "\n",
    "    for i in range(K):\n",
    "        for j, doc in enumerate(corpus):\n",
    "            phi_d = corpus_free_params[j][0]\n",
    "            for n, v in enumerate(doc):\n",
    "                a = (phi_d[n, i])\n",
    "                beta[i, int(v)] += a\n",
    "        beta[i] /= np.full((1, V), sum(beta[i]))[0]\n",
    "\n",
    "    for j in range(len(corpus)):\n",
    "        mu += corpus_free_params[j][2]\n",
    "    mu /= np.full((1, K), len(corpus))[0]\n",
    "\n",
    "    for j in range(len(corpus)):\n",
    "        lam_d = corpus_free_params[j][2]\n",
    "        nu_d = corpus_free_params[j][1]\n",
    "        lam_mu_dif = (lam_d - mu)\n",
    "        sigma += np.diag(nu_d)+np.outer((lam_mu_dif), np.transpose(lam_mu_dif))\n",
    "    sigma /= np.full((K, K), len(corpus))[0]\n",
    "\n",
    "    return mu, sigma, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3321d",
   "metadata": {},
   "source": [
    "## arXiv.org API interface\n",
    "Here, we can specify how many documents we want to scrape and how many documents we want to include given each author. Further, preprocessing and cleaning as well as extraction of author names, document subject classifications (the meta-tags) and titles as well as abstracts is handled here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fab4de7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ym/47xk9jjx7_qb9zbpnylll5nm0000gn/T/ipykernel_10691/284819307.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sw.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ym/47xk9jjx7_qb9zbpnylll5nm0000gn/T/ipykernel_10691/284819307.py\u001b[0m in \u001b[0;36mgenerate_stopwords\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mraw_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\s+'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stopwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0msw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_stopwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_stopwords(dir):\n",
    "    sw = []\n",
    "    raw_stopwords = np.array(pd.read_csv(dir, header=None, sep='\\s+'))\n",
    "    for s in range(len(raw_stopwords)):\n",
    "        sw.append(str(raw_stopwords[s])[2:-2])\n",
    "    return sw\n",
    "\n",
    "\n",
    "stopwords = generate_stopwords(\"sw.txt\")\n",
    "\n",
    "\n",
    "def clean_string(s):\n",
    "    s = s.translate(str.maketrans('', '', \"()%[]{}\\'\\\"@.,:;?!=+/\\\\&`*#$£€_^'0123456789~\"))\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_cat(s):\n",
    "    r = []\n",
    "    c = str(s)\n",
    "    count = c.count('/schemas/atom\" term=\"')\n",
    "    c = c.replace(str('/schemas/atom\" term=\"'), \"DSFGC\")\n",
    "    c = c.replace(str('scheme=\"http://arxiv.org/schemas/atom\"/>'), \"DSFGC\")\n",
    "    #print(c)\n",
    "    #for i in range(count):\n",
    "    c = c.split(\"DSFGC\", 2)\n",
    "    r.append((c[1]).translate(str.maketrans('', '', ' \\'\\\"')))\n",
    "    #c = c[2]\n",
    "    #print(r)\n",
    "    return r\n",
    "\n",
    "\n",
    "def corpus_by_single_author(au, number):\n",
    "    c = []\n",
    "    cats = []\n",
    "    #au:M+AND+au:Blei for the author David M. Blei\n",
    "    url = 'http://export.arxiv.org/api/query?search_query='+au+'&start=0&max_results='+str(number)\n",
    "    data = request.get(url)\n",
    "    data = str(data.content)\n",
    "    data = data.replace('<title>', 'XSX')\n",
    "    data = data.replace('</summary>', 'XSX')\n",
    "    data = data.replace('<entry>', 'DSFGC')\n",
    "    data_split = data.split('DSFGC')\n",
    "\n",
    "    #data = clean_string(data)\n",
    "    for i in range(len(data_split)-1):\n",
    "        cat = get_cat(data_split[i+1])\n",
    "        cats.append(cat)\n",
    "        data_split_part = data_split[i+1].split('XSX')\n",
    "        title_abstract = str(data_split_part[1])\n",
    "        title_abstract = title_abstract.replace('</title>', ' ')\n",
    "        title_abstract = title_abstract.replace('<summary>', ' ')\n",
    "        title_abstract = title_abstract.replace('\\\\n', ' ')\n",
    "\n",
    "        #print(title_abstract)\n",
    "        c.append(clean_string(title_abstract))\n",
    "    cats_clean = {}\n",
    "    for j, i in enumerate(cats):\n",
    "        if i[0] in cats_clean:\n",
    "            cats_clean[i[0]] += 1 / len(cats)\n",
    "        else:\n",
    "            cats_clean[i[0]] = 1 / len(cats)\n",
    "    #for i in range(len(data)):\n",
    "    #    if i % 2 == 0:\n",
    "    #        data[i] = 0\n",
    "    #    else:\n",
    "    #\n",
    "    #        data[i] = data[i].replace(\"<title>n    <summary>\", \" \")\n",
    "    #        c.append(clean_string(data[i]))\n",
    "    #print(input)\n",
    "    #print(len(c))\n",
    "    return c, len(c), cats_clean\n",
    "\n",
    "\n",
    "def extract_xml_elements(element, s):\n",
    "    r = []\n",
    "    elementEnd = '</'+str(element[1:])\n",
    "    c = str(s)\n",
    "    count = c.count(element)\n",
    "    c = c.replace(str(element), \"DSFGC\")\n",
    "    c = c.replace(str(elementEnd), \"DSFGC\")\n",
    "\n",
    "    for i in range(count):\n",
    "        c = c.split(\"DSFGC\", 2)\n",
    "        r.append(c[1])\n",
    "        c = c[2]\n",
    "    return r\n",
    "\n",
    "def get_authors(input, limit):\n",
    "    authors = [] # ATTENTION ! CHANGED TO 'all:' FROM 'jr:'\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=all:'+input+'&sortBy=lastUpdatedDate&start=0&max_results='+str(limit)\n",
    "    data = request.get(url)\n",
    "    data = str(data.content)\n",
    "    data = data.replace('<entry>','DSFGC')\n",
    "    data = data.split('DSFGC')\n",
    "    for i in range(len(data)-1):\n",
    "        a = extract_xml_elements('<name>', data[i+1])\n",
    "        authors.append(a)\n",
    "\n",
    "    #print(authors)\n",
    "    return authors\n",
    "\n",
    "def author_to_query(input):\n",
    "    au = clean_string(input)\n",
    "    au = au.split()\n",
    "    aq = 'au:%22' + au[0]\n",
    "    for j in range(len(au) - 2):\n",
    "        aq += ' ' + au[j+1]\n",
    "    aq += ' ' + au[-1] + '%22'\n",
    "    return aq\n",
    "\n",
    "\n",
    "def corpus_by_authors():\n",
    "    c = []\n",
    "    meta = []\n",
    "    authors = get_authors('e', 5)\n",
    "    duplicates = []\n",
    "    for a in range(len(authors)):\n",
    "\n",
    "        for b in authors[a][0:3]:\n",
    "            aa = b.split('\\\\', 1)[0]\n",
    "            aa = str(aa).rsplit(' ', 1)[-1]\n",
    "            if aa not in duplicates:\n",
    "                try:\n",
    "                    q = author_to_query(b)\n",
    "                except:\n",
    "                    q = 'nobody'\n",
    "                res = corpus_by_single_author(str(q), 15)\n",
    "                cor, count, cat = res\n",
    "                if count > 1:\n",
    "                    meta.append((b, count, cat))\n",
    "                    c.append(cor)\n",
    "                    duplicates.append(aa)\n",
    "    #print(meta)\n",
    "    print(len(meta))\n",
    "    #np.savetxt('authors.txt', meta, fmt='%s')\n",
    "    return c\n",
    "\n",
    "\n",
    "docs = corpus_by_authors()\n",
    "print(docs)\n",
    "docs = np.concatenate(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10f306",
   "metadata": {},
   "source": [
    "## Main function\n",
    "The main() function stands as the confluence of all above shown code snippits and weaves it into one coherent process. It first sorts and processes all words and creates the Bag-of-words, creates a *dict* for all words and goes on to execute the EM-Algorithm until the relative chage in the ELBO is below the threshold. It saves the results via *np.savetxt()*. These .txt files are important as they are later used for the interactive browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f118182a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ym/47xk9jjx7_qb9zbpnylll5nm0000gn/T/ipykernel_10691/3027043589.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/ym/47xk9jjx7_qb9zbpnylll5nm0000gn/T/ipykernel_10691/3027043589.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mreappearance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mrow_of_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrow_of_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrow_of_text\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    index = 0\n",
    "    reappearance = {}\n",
    "    for doc in docs:\n",
    "        row_of_text = doc.split()\n",
    "        row_of_text = [word for word in row_of_text if not word.lower() in stopwords]\n",
    "        row_of_text = [word for word in row_of_text if not len(word) > 18 or len(word) == 0]\n",
    "\n",
    "        row_of_IDs = np.zeros(len(row_of_text))\n",
    "        for v, word in enumerate(row_of_text):\n",
    "            if word.lower() in words:\n",
    "                reappearance[word.lower()] += 1\n",
    "            else:\n",
    "                words[word.lower()] = int(index)\n",
    "                words_inv[int(index)] = word.lower()\n",
    "                reappearance[word.lower()] = 1\n",
    "                index += 1\n",
    "            row_of_IDs[v] = int(words[word.lower()])\n",
    "        corpus_text.append(row_of_text)\n",
    "        corpus.append(row_of_IDs)\n",
    "    # unique_word_ratio = index/(index+reappearance)\n",
    "    global V\n",
    "    V = len(words)\n",
    "    print(words)\n",
    "    print(reappearance)\n",
    "\n",
    "    global latent_param\n",
    "    latent_param = init_latent_param(V)\n",
    "    i = 0\n",
    "    elbo_old = 1\n",
    "    save_words_inv = [words_inv]\n",
    "    #np.savetxt('words_inv.txt', save_words_inv, fmt='%s')\n",
    "    corpus_free_params_old = 0\n",
    "    while True:\n",
    "        print('iter: ', i, 'elbo: ', elbo_old)\n",
    "        corpus_free_params, elbo = expectation_step(corpus, corpus_free_params_old)\n",
    "        convergence = (elbo_old-elbo)/elbo_old\n",
    "        print('relative change: ', convergence, '\\n')\n",
    "        mu, sigma, beta = maximization_step(corpus_free_params)\n",
    "        latent_param = mu, sigma, beta\n",
    "        corpus_free_params_old = corpus_free_params\n",
    "        #save numbers at every interation\n",
    "        beta_full = []\n",
    "        mu_full = []\n",
    "        sigma_full = []\n",
    "        for j in range(K):\n",
    "            mu_full.append(latent_param[0][j])\n",
    "            sigma_full.append(latent_param[1][j])\n",
    "            beta_full.append(latent_param[2][j])\n",
    "        #np.savetxt('beta_numbers.txt', beta_full, fmt='%s')\n",
    "        #np.savetxt('sigma_numbers.txt', sigma_full, fmt='%s')\n",
    "        #np.savetxt('mu_numbers.txt', mu_full, fmt='%s')\n",
    "\n",
    "        lam_raw = []\n",
    "        for j in range(len(corpus)):\n",
    "            lam_raw.append(corpus_free_params[j][2])\n",
    "        #np.savetxt(\"lam_raw.txt\", lam_raw)\n",
    "        i += 1\n",
    "        if convergence > 0.0005 and i < 1000:\n",
    "            elbo_old = elbo\n",
    "            if elbo_old > elbo and i > 4:\n",
    "                print(\"WARNING! - DIVERGENCE\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182b862",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
