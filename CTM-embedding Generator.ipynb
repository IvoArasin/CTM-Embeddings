{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b6a908d",
   "metadata": {},
   "source": [
    "# CTM-Embeddings\n",
    "The below code comprises both the CTM algorithm itself as well as the fetching and extraction of necessary data from the (arXiv.org) API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2c49ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary libraries\n",
    "import numpy as np\n",
    "import requests as request\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eba70c9",
   "metadata": {},
   "source": [
    "## Correlated Topic Model Algorithm\n",
    "\n",
    "\n",
    "Below section is comprised of:\n",
    "1. Evidence Lower Bound - ELBO computation\n",
    "2. Optimisation of document scope local variable Zeta that is part of the additional variational inference approximation within the variational inference for the latent variables of a document\n",
    "3. Optimisation of Phi, the free parameter of the variational density for topic-assignment variable z\n",
    "4. Optimisation of Lambda, the mean of the variational density for latent document-topic variable Eta\n",
    "5. Optimisation of Nu, the variance of the variational density for latent document-topic variable Eta\n",
    "6. Random initialisation of the free parameters of the variational densities\n",
    "7. Random initialisation of the latent global model parameters \n",
    "8. Local optimisation of one document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04aa45d7",
   "metadata": {},
   "source": [
    "## Defining global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0537fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_text = []  # documents in words\n",
    "corpus = []  # documents in IDs\n",
    "words = {}  # word : ID\n",
    "words_inv = {}  # ID : word\n",
    "K = 25  # number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7c546",
   "metadata": {},
   "source": [
    "## Evidence Lower Bound\n",
    "Evidence Lower Bound (ELBO) Computation\n",
    "Computation for one document at a time.\n",
    "Formula as follows (taken from Blei and Lafferty (2007)):\n",
    "$ELBO: E_q[\\log p(\\eta|\\mu,\\Sigma)]+\\sum_{n=0}^N(E_q[\\log p(z_n|\\eta)]+E_q[\\log p(w_n|z_n,\\beta)])+H(q)$\n",
    "\n",
    "*where*\n",
    "\n",
    "$E_q[\\log p(\\eta|\\mu,\\Sigma)] = \\frac{1}{2}\\log det(\\Sigma^{-1})-\\frac{K}{2}\\log 2\\pi-\\frac{1}{2}(tr(diag(\\nu^2_j)\\Sigma^{-1})+(\\lambda_j-\\mu)^T\\Sigma^{-1}(\\lambda_j-\\mu))$\n",
    "\n",
    "$E_q[\\log p(z_n|\\eta)] = \\sum_{i=0}^K\\lambda_{j,i}\\phi_{j,n,i}-\\zeta^{-1}_j\\big(\\sum_{i=0}^K e^{\\lambda_{_{}^{j,i}}+\\frac{\\nu^2_{j,i}}{2}}\\big)+1-\\log\\zeta_j$\n",
    "\n",
    "$E_q[\\log p(w_n|z_n,\\beta)] = \\sum_{i=0}^K\\phi_{j,n,i}\\log \\beta_{i,w_n}$\n",
    "\n",
    "$H(q) = \\sum_{i=0}^K\\frac{1}{2}(\\log \\nu^2_{j,i}+\\log 2\\pi +1)-\\sum_{n=0}^N\\sum_{i=0}^K\\phi_{j,n,i}\\log \\phi_{j,n,i}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269b2b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ELBO(latent_param, free_param, doc):\n",
    "    mu, sigma, beta = latent_param\n",
    "    zeta, phi, nu, lam = free_param\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    lam_mu_diff = (lam-mu)\n",
    "    term_1 = 0.5*np.log(np.linalg.det(sigma_inv))-(K/2)*np.log(2*np.pi)\\\n",
    "             -(0.5*np.trace(np.dot(np.diag(nu), sigma_inv))\n",
    "               +np.dot(np.dot(np.transpose(lam_mu_diff), sigma_inv), lam_mu_diff))\n",
    "    term_2 = 0\n",
    "    term_2_1 = -(1/zeta)*sum(np.exp(lam+nu/2))+1-np.log(zeta)\n",
    "    term_3 = 0\n",
    "    term_4 = 0.5*sum(np.log(nu)+np.log(2*np.pi)+1)\n",
    "    term_4_1 = 0\n",
    "\n",
    "    for n, word in enumerate(doc):\n",
    "        term_2 += np.dot(lam, phi[n])\n",
    "        term_3 += np.dot(phi[n], np.log(beta[:, int(word)]))\n",
    "        term_4_1 += np.dot(phi[n], np.log(phi[n]))\n",
    "\n",
    "    term_2 += len(doc)*term_2_1\n",
    "    term_4 -= term_4_1\n",
    "    res = term_1 + term_2 + term_3 + term_4\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172e5b81",
   "metadata": {},
   "source": [
    "## Optimisation of Zeta\n",
    "\n",
    "$\\hat\\zeta_j =\\sum_{i=1}^K e^{\\big(\\lambda_i +\\frac{\\nu_i^2}{2}\\big)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce421f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_zeta(free_param):\n",
    "\n",
    "    zeta, phi, nu, lam = free_param\n",
    "    zeta = sum(np.exp(lam+nu/2))\n",
    "\n",
    "    return zeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551098b8",
   "metadata": {},
   "source": [
    "## Optimisation of Phi\n",
    "\n",
    "$\\overset{\\rightarrow}{\\hat\\phi_{j,n}}=\\frac{\\hat\\phi_{n,i}}{\\sum_{i=0}^K\\hat\\phi_{n,i}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb2509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_phi(latent_param, free_param, doc):\n",
    "    mu, sigma, beta = latent_param\n",
    "    zeta, phi, nu, lam = free_param\n",
    "\n",
    "    phi = np.ones((len(doc), K))\n",
    "    for n, word in enumerate(doc):\n",
    "        summation = 0\n",
    "        for i in range(K):\n",
    "\n",
    "            phi[n, i] = np.exp(lam[i])*beta[i, int(word)]\n",
    "            summation += phi[n, i]\n",
    "        phi[n] = phi[n]/summation\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932d8db",
   "metadata": {},
   "source": [
    "## Optimisation of Lambda\n",
    "As proposed in the original paper, a conjugate-gradient algorithm is used for optimisation.\n",
    "Here we apply the 'Newton-GC' algorithm provided in the scipy.optimize library.\n",
    "\n",
    "$\\frac{\\partial ELBO}{\\partial \\overset{\\rightarrow}\\lambda}: -\\Sigma^{-1}({\\overset{\\rightarrow}\\lambda-\\overset{\\rightarrow}{\\mu}})+\\sum_{n=1}^N\\overset{\\rightarrow}{\\hat\\phi_{n}}-\\frac{N}{\\zeta}e^{{\\overset{\\rightarrow}\\lambda}+\\frac{{\\overset{\\rightarrow}\\nu^2}}{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8554a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_lam(latent_param, free_param, doc):\n",
    "    mu, sigma, beta = latent_param\n",
    "    zeta, phi, nu, lam = free_param\n",
    "\n",
    "    def elbo_df_lam(x):\n",
    "        return np.dot(np.linalg.inv(sigma), (x-mu))-sum(phi)+(len(doc)/zeta)*np.exp(x+nu/2)\n",
    "\n",
    "    def elbo_lam(x):\n",
    "        return -ELBO(latent_param, (zeta, phi, nu, x), doc)\n",
    "\n",
    "    optimized_lambda = minimize(fun=elbo_lam, x0=lam, jac=elbo_df_lam, method='Newton-CG', options={'disp': 0,'xtol': 0.00001})\n",
    "\n",
    "    return optimized_lambda.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc420a5",
   "metadata": {},
   "source": [
    "## Optimisation of Nu\n",
    "Nu is optimised via a variant of the classic Newton-Raphson optimisation.\n",
    "\n",
    "$f(\\nu^2_i) = \\frac{\\partial L}{\\partial \\nu^2} : -\\Sigma_{ii}^{-1}\\frac{1}{2}-\\frac{N}{2\\zeta}e^{\\lambda_i+\\frac{\\nu^2_i}{2}}+\\frac{1}{2\\nu^2_i}$\n",
    "\n",
    "and\n",
    "\n",
    "$f$**'**$(\\nu^2_i) = \\frac{\\partial L^2}{\\partial^2 \\nu^2} : -\\frac{N}{4\\zeta}e^{\\lambda_i+\\frac{\\nu^2_i}{2}}-\\frac{1}{2}(\\nu^2_i)^{-2}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_nu(latent_param, free_param, doc):\n",
    "    mu, sigma, beta = latent_param\n",
    "    zeta, phi, nu, lam = free_param\n",
    "    N = len(doc)\n",
    "    sigma_inv = np.linalg.inv(sigma)\n",
    "    df_1 = lambda x, i, s_inv, l: -0.5*s_inv-0.5*(N/zeta)*np.exp(l + x/2)+1/(2*x)\n",
    "    df_2 = lambda x, i, l: -0.25*(N/zeta)*np.exp(l+x/2)-0.5*(1/(x*x))\n",
    "\n",
    "    for i in range(K):\n",
    "        s_inv = sigma_inv[i, i]\n",
    "        l = lam[i]\n",
    "        x = 5\n",
    "        log_x = np.log(x)\n",
    "        df1 = 1\n",
    "        x_safety = 10\n",
    "        while np.abs(df1) > 0.0001:\n",
    "            if np.isnan(x):\n",
    "                x_safety *= 2  # Two was used in the original C-code provided by the paper itself\n",
    "                x = x_safety\n",
    "                log_x = np.log(x)\n",
    "                print(\"nan detected, value set to \", x)\n",
    "\n",
    "            x = np.exp(log_x)\n",
    "            df1 = df_1(x, i, s_inv, l)\n",
    "            df2 = df_2(x, i, l)\n",
    "            log_x -= (x*df1) / (x*x*df2+x*df1)\n",
    "        nu[i] = np.exp(log_x)\n",
    "    return nu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bb641b",
   "metadata": {},
   "source": [
    "## Random initialisation of free parameters of variational distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba41bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_free_param(doc):\n",
    "    zeta = 2 #Zeta set to Two as was done in the original paper Blei and Lafferty (2006)\n",
    "    phi = np.random.dirichlet(np.ones(K), len(doc))\n",
    "    nu = np.ones(K)\n",
    "    lam = np.zeros(K)\n",
    "    return zeta, phi, nu, lam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d5a4a",
   "metadata": {},
   "source": [
    "## Random initialisation of latent model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277ffc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_latent_param(V):\n",
    "    mu = np.zeros(K)\n",
    "    sigma = np.eye(K)\n",
    "    beta = np.random.dirichlet(np.ones(V), K)\n",
    "    return mu, sigma, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c305d5f",
   "metadata": {},
   "source": [
    "## Optimisation of document\n",
    "Convergence determined by relative change in ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f46db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_doc(latent_param, doc, free_param):\n",
    "\n",
    "    zeta, phi, nu, lam = free_param\n",
    "    bound_old = ELBO(latent_param, free_param, doc)\n",
    "    iters = 0\n",
    "\n",
    "    while True:\n",
    "        iters += 1\n",
    "        zeta = opt_zeta(free_param)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        lam = opt_lam(latent_param, free_param, doc)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        zeta = opt_zeta(free_param)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        nu = opt_nu(latent_param, free_param, doc)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        zeta = opt_zeta(free_param)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "        phi = opt_phi(latent_param, free_param, doc)\n",
    "        free_param = zeta, phi, nu, lam\n",
    "\n",
    "\n",
    "        bound = ELBO(latent_param, free_param, doc)\n",
    "\n",
    "        if np.abs((bound_old-bound)/bound_old) > 0.00001 and iters < 500:\n",
    "            bound_old = bound\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return zeta, phi, nu, lam, bound_old"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b89acba",
   "metadata": {},
   "source": [
    "## Expectation Step\n",
    "Here, all documents are optimised w.r.t ELBO and passed to the Maximisation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1ebe5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expectation_step(corpus, old):\n",
    "\n",
    "    corpus_free_params = []\n",
    "    elbo_sum = 0\n",
    "    for i, doc in enumerate(corpus):\n",
    "        if old == 0:\n",
    "            free_param = random_free_param(doc)\n",
    "        else:\n",
    "            zeta = 2\n",
    "            phi = old[i][0]\n",
    "            nu = old[i][1]\n",
    "            lam = old[i][2]\n",
    "            free_param = zeta, phi, nu, lam\n",
    "        values = opt_doc(latent_param, doc, free_param=free_param)\n",
    "        zeta, phi, nu, lam, bound_old = values\n",
    "        corpus_free_params.append((phi.copy(), nu.copy(), lam.copy()))\n",
    "        elbo_sum += bound_old\n",
    "    return corpus_free_params, elbo_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c337f2",
   "metadata": {},
   "source": [
    "## Maximisation Step\n",
    "Here, the global scoped latent model parameters (Mu, Sigma, Beta) are optimised w.r.t ELBO given all the document-scope free  parameters from the optimised documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79720f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximization_step(corpus_free_params):\n",
    "\n",
    "    mu, sigma, beta = latent_param\n",
    "\n",
    "    for i in range(K):\n",
    "        for j, doc in enumerate(corpus):\n",
    "            phi_d = corpus_free_params[j][0]\n",
    "            for n, v in enumerate(doc):\n",
    "                a = (phi_d[n, i])\n",
    "                beta[i, int(v)] += a\n",
    "        beta[i] /= np.full((1, V), sum(beta[i]))[0]\n",
    "\n",
    "    for j in range(len(corpus)):\n",
    "        mu += corpus_free_params[j][2]\n",
    "    mu /= np.full((1, K), len(corpus))[0]\n",
    "\n",
    "    for j in range(len(corpus)):\n",
    "        lam_d = corpus_free_params[j][2]\n",
    "        nu_d = corpus_free_params[j][1]\n",
    "        lam_mu_dif = (lam_d - mu)\n",
    "        sigma += np.diag(nu_d)+np.outer((lam_mu_dif), np.transpose(lam_mu_dif))\n",
    "    sigma /= np.full((K, K), len(corpus))[0]\n",
    "\n",
    "    return mu, sigma, beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3321d",
   "metadata": {},
   "source": [
    "## arXiv.org API interface\n",
    "Here, we can specify how many documents we want to scrape and how many documents we want to include given each author. Further, preprocessing and cleaning as well as extraction of author names, document subject classifications (the meta-tags) and titles as well as abstracts is handled here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b5484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stopwords(dir):\n",
    "    sw = []\n",
    "    raw_stopwords = np.array(pd.read_csv(dir, header=None, sep='\\s+'))\n",
    "    for s in range(len(raw_stopwords)):\n",
    "        sw.append(str(raw_stopwords[s])[2:-2])\n",
    "    return sw\n",
    "\n",
    "\n",
    "stopwords = generate_stopwords(\"sw.txt\")\n",
    "\n",
    "\n",
    "def clean_string(s):\n",
    "    s = s.translate(str.maketrans('', '', \"()%[]{}\\'\\\"@.,:;?!=+/\\\\&`*#$£€_^'0123456789~\"))\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_cat(s):\n",
    "    r = []\n",
    "    c = str(s)\n",
    "    count = c.count('/schemas/atom\" term=\"')\n",
    "    c = c.replace(str('/schemas/atom\" term=\"'), \"DSFGC\")\n",
    "    c = c.replace(str('scheme=\"http://arxiv.org/schemas/atom\"/>'), \"DSFGC\")\n",
    "    #print(c)\n",
    "    #for i in range(count):\n",
    "    c = c.split(\"DSFGC\", 2)\n",
    "    r.append((c[1]).translate(str.maketrans('', '', ' \\'\\\"')))\n",
    "    #c = c[2]\n",
    "    #print(r)\n",
    "    return r\n",
    "\n",
    "\n",
    "def corpus_by_single_author(au, number):\n",
    "    c = []\n",
    "    cats = []\n",
    "    #au:M+AND+au:Blei for the author David M. Blei\n",
    "    url = 'http://export.arxiv.org/api/query?search_query='+au+'&start=0&max_results='+str(number)\n",
    "    data = request.get(url)\n",
    "    data = str(data.content)\n",
    "    data = data.replace('<title>', 'XSX')\n",
    "    data = data.replace('</summary>', 'XSX')\n",
    "    data = data.replace('<entry>', 'DSFGC')\n",
    "    data_split = data.split('DSFGC')\n",
    "\n",
    "    #data = clean_string(data)\n",
    "    for i in range(len(data_split)-1):\n",
    "        cat = get_cat(data_split[i+1])\n",
    "        cats.append(cat)\n",
    "        data_split_part = data_split[i+1].split('XSX')\n",
    "        title_abstract = str(data_split_part[1])\n",
    "        title_abstract = title_abstract.replace('</title>', ' ')\n",
    "        title_abstract = title_abstract.replace('<summary>', ' ')\n",
    "        title_abstract = title_abstract.replace('\\\\n', ' ')\n",
    "\n",
    "        #print(title_abstract)\n",
    "        c.append(clean_string(title_abstract))\n",
    "    cats_clean = {}\n",
    "    for j, i in enumerate(cats):\n",
    "        if i[0] in cats_clean:\n",
    "            cats_clean[i[0]] += 1 / len(cats)\n",
    "        else:\n",
    "            cats_clean[i[0]] = 1 / len(cats)\n",
    "    #for i in range(len(data)):\n",
    "    #    if i % 2 == 0:\n",
    "    #        data[i] = 0\n",
    "    #    else:\n",
    "    #\n",
    "    #        data[i] = data[i].replace(\"<title>n    <summary>\", \" \")\n",
    "    #        c.append(clean_string(data[i]))\n",
    "    #print(input)\n",
    "    #print(len(c))\n",
    "    return c, len(c), cats_clean\n",
    "\n",
    "\n",
    "def extract_xml_elements(element, s):\n",
    "    r = []\n",
    "    elementEnd = '</'+str(element[1:])\n",
    "    c = str(s)\n",
    "    count = c.count(element)\n",
    "    c = c.replace(str(element), \"DSFGC\")\n",
    "    c = c.replace(str(elementEnd), \"DSFGC\")\n",
    "\n",
    "    for i in range(count):\n",
    "        c = c.split(\"DSFGC\", 2)\n",
    "        r.append(c[1])\n",
    "        c = c[2]\n",
    "    return r\n",
    "\n",
    "def get_authors(input, limit):\n",
    "    authors = [] # ATTENTION ! CHANGED TO 'all:' FROM 'jr:'\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=all:'+input+'&sortBy=lastUpdatedDate&start=0&max_results='+str(limit)\n",
    "    data = request.get(url)\n",
    "    data = str(data.content)\n",
    "    data = data.replace('<entry>','DSFGC')\n",
    "    data = data.split('DSFGC')\n",
    "    for i in range(len(data)-1):\n",
    "        a = extract_xml_elements('<name>', data[i+1])\n",
    "        authors.append(a)\n",
    "\n",
    "    #print(authors)\n",
    "    return authors\n",
    "\n",
    "def author_to_query(input):\n",
    "    au = clean_string(input)\n",
    "    au = au.split()\n",
    "    aq = 'au:%22' + au[0]\n",
    "    for j in range(len(au) - 2):\n",
    "        aq += ' ' + au[j+1]\n",
    "    aq += ' ' + au[-1] + '%22'\n",
    "    return aq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdfb8d",
   "metadata": {},
   "source": [
    "\n",
    "The *corpus_by_authors()* specifies the volume of retrieved documents \n",
    "and how many documents per author should be used. It aggregates all documents into the *docs* variable in the bottom of the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab4de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_by_authors():\n",
    "    c = []\n",
    "    meta = []\n",
    "    authors = get_authors('e', 250)#How many documents to retrieve from which all names are taken to scrape docs from these authors\n",
    "    duplicates = []\n",
    "    for a in range(len(authors)):\n",
    "\n",
    "        for b in authors[a][0:3]:\n",
    "            aa = b.split('\\\\', 1)[0]\n",
    "            aa = str(aa).rsplit(' ', 1)[-1]\n",
    "            if aa not in duplicates:\n",
    "                try:\n",
    "                    q = author_to_query(b)\n",
    "                except:\n",
    "                    q = 'nobody'\n",
    "                res = corpus_by_single_author(str(q), 15)#Max. 15 docs per author\n",
    "                cor, count, cat = res\n",
    "                if count > 9: #Min. NR of docs per author\n",
    "                    meta.append((b, count, cat))\n",
    "                    c.append(cor)\n",
    "                    duplicates.append(aa)\n",
    "    #print(meta)\n",
    "    print(len(meta))\n",
    "    np.savetxt('authors.txt', meta, fmt='%s')\n",
    "    return c\n",
    "\n",
    "\n",
    "docs = corpus_by_authors()\n",
    "docs = np.concatenate(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10f306",
   "metadata": {},
   "source": [
    "## Main function\n",
    "The main() function stands as the confluence of all above shown code snippits and weaves it into one coherent process. It first sorts and processes all words and creates the Bag-of-words, creates a *dict* for all words and goes on to execute the EM-Algorithm until the relative chage in the ELBO is below the threshold. It saves the results via *np.savetxt()*. These .txt files are important as they are later used for the interactive browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f118182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    index = 0\n",
    "    reappearance = {}\n",
    "    for doc in docs:\n",
    "        row_of_text = doc.split()\n",
    "        row_of_text = [word for word in row_of_text if not word.lower() in stopwords]\n",
    "        row_of_text = [word for word in row_of_text if not len(word) > 18 or len(word) == 0]\n",
    "\n",
    "        row_of_IDs = np.zeros(len(row_of_text))\n",
    "        for v, word in enumerate(row_of_text):\n",
    "            if word.lower() in words:\n",
    "                reappearance[word.lower()] += 1\n",
    "            else:\n",
    "                words[word.lower()] = int(index)\n",
    "                words_inv[int(index)] = word.lower()\n",
    "                reappearance[word.lower()] = 1\n",
    "                index += 1\n",
    "            row_of_IDs[v] = int(words[word.lower()])\n",
    "        corpus_text.append(row_of_text)\n",
    "        corpus.append(row_of_IDs)\n",
    "    # unique_word_ratio = index/(index+reappearance)\n",
    "    global V\n",
    "    V = len(words)\n",
    "    print(words)\n",
    "    print(reappearance)\n",
    "\n",
    "    global latent_param\n",
    "    latent_param = init_latent_param(V)\n",
    "    i = 0\n",
    "    elbo_old = 1\n",
    "    save_words_inv = [words_inv]\n",
    "    np.savetxt('words_inv.txt', save_words_inv, fmt='%s')\n",
    "    corpus_free_params_old = 0\n",
    "    while True:\n",
    "        print('iter: ', i, 'elbo: ', elbo_old)\n",
    "        corpus_free_params, elbo = expectation_step(corpus, corpus_free_params_old)\n",
    "        convergence = (elbo_old-elbo)/elbo_old\n",
    "        print('relative change: ', convergence, '\\n')\n",
    "        mu, sigma, beta = maximization_step(corpus_free_params)\n",
    "        latent_param = mu, sigma, beta\n",
    "        corpus_free_params_old = corpus_free_params\n",
    "        #save numbers at every interation\n",
    "        beta_full = []\n",
    "        mu_full = []\n",
    "        sigma_full = []\n",
    "        for j in range(K):\n",
    "            mu_full.append(latent_param[0][j])\n",
    "            sigma_full.append(latent_param[1][j])\n",
    "            beta_full.append(latent_param[2][j])\n",
    "        np.savetxt('beta_numbers.txt', beta_full, fmt='%s')\n",
    "        lam_raw = []\n",
    "        for j in range(len(corpus)):\n",
    "            lam_raw.append(corpus_free_params[j][2])\n",
    "        np.savetxt(\"lam_raw.txt\", lam_raw)\n",
    "        i += 1\n",
    "        if convergence > 0.0005 and i < 1000:\n",
    "            elbo_old = elbo\n",
    "            if elbo_old > elbo and i > 4:\n",
    "                print(\"WARNING! - DIVERGENCE\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    return\n",
    "\n",
    "\n",
    "#main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3287893e",
   "metadata": {},
   "source": [
    "\n",
    "#### Code above created files used for the interactive browser:\n",
    "The above code creates important files used for the author-embedding browser.\n",
    "It creates: \"beta_numbers.txt\", \"words_inv.txt\", \"lam_raw.txt\" and \"authors.txt\" and must be made available when running the script for the interactive browser.\n",
    "(The files are provided in the repository as well, so no need to run this code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e96d419",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "As a reference guide, I have used the C-implementation of Correlated Topic Model found in the official repository of David Blei (https://github.com/blei-lab/ctm-c)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
