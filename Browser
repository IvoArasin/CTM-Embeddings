{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "689a5300",
   "metadata": {},
   "source": [
    "# Author-Embedding Browser\n",
    "\n",
    "This script creates the author-embeddings via the data generated with CTM (see provided script).\n",
    "It provides a web-based interface to interact with the author-embeddings in a retrieval setting.\n",
    "\n",
    "Given that GitHub can only serve static Jupyter Notebook, a working version of the interactive browser can be found here (http://ivoarasin.pythonanywhere.com/). It uses the exact same code found below and the data generated via the CTM script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84607e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import plotly.graph_objs as go\n",
    "from dash.dependencies import Input, Output, State\n",
    "import random\n",
    "from dash.exceptions import PreventUpdate\n",
    "import time\n",
    "\n",
    "#Number of topics\n",
    "K = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202b2df9",
   "metadata": {},
   "source": [
    "\n",
    "## Computation of topic-correlation and construction of graph\n",
    "Computation of topical correlation via Lasso-Regression as explained in the main work or as explained briefly in Blei and Lafferty (2007)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7c727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(x, size, alpha):\n",
    "    r = list(np.zeros(size))\n",
    "    for i in range(size):\n",
    "        r[i] = int(i)\n",
    "\n",
    "    x = np.matrix(x)\n",
    "    Shat = np.full((size, size), 0)\n",
    "    LAS = Lasso(alpha=alpha, tol=0.0001, fit_intercept=True)\n",
    "    for i in range(size):\n",
    "        r2 = r.copy()\n",
    "        r2.remove(i)\n",
    "        y = x[:, i]\n",
    "        X = x[:, r2]\n",
    "        out = LAS.fit(X, y)\n",
    "        coefs = out.coef_\n",
    "        gap = 0\n",
    "        for j in range(size):\n",
    "            if j == i:\n",
    "                Shat[i, i] = 1\n",
    "                gap += 1\n",
    "            else:\n",
    "                if np.abs(coefs[j-gap]) > 0:\n",
    "                    Shat[i, j] = 1\n",
    "    G = nx.from_numpy_matrix(np.array(Shat))\n",
    "    return G\n",
    "\n",
    "def draw_graph(G, color_map, size_map):\n",
    "    nx.draw(G, node_color=color_map, node_size=size_map, with_labels=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989d92fc",
   "metadata": {},
   "source": [
    "\n",
    "## Some miscellaneous function used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d24d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(s):\n",
    "    s = s.translate(str.maketrans('', '', '[]{}\\'\\\"@.,:;?!=-+/\\\\&`*#^'))\n",
    "    return s\n",
    "\n",
    "\n",
    "def scale(x):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = (x[i]-np.mean(x[i]))/(np.full( (1,len(x[i])), np.var(x[i])**0.5)[0])\n",
    "    return x\n",
    "\n",
    "\n",
    "def read(path):\n",
    "    data = np.matrix(pd.read_csv(path, header=None))\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_proper(data, length):\n",
    "    c = np.zeros((len(data), length))\n",
    "    for i in range(len(data)):\n",
    "        a = data[i, 0].split()\n",
    "        b = [float(n) for n in a]\n",
    "        c[i] = b\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e6538",
   "metadata": {},
   "source": [
    "\n",
    "## Fetching and transformation of data provided by CTM\n",
    "This generates the author-embeddings and all the data necessary for the browser. It need access to all files generated by the CTM-script, e.g. 'words_inv.txt\", \"authors.txt\", beta_numbers.txt\" and \"lam_raw.txt\" (also provided in repository)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b2273",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret():\n",
    "    # scaled Lambda\n",
    "    # Theta\n",
    "    # author embeddings\n",
    "    # Betas as topic-word distributions\n",
    "    # word_inv list\n",
    "\n",
    "    word_inv_raw = str(pd.read_csv('words_inv.txt', delimiter='\\t'))\n",
    "    word_inv_raw = word_inv_raw.split()\n",
    "    words_inv = []\n",
    "    ind = 3\n",
    "    for i in range(int(len(word_inv_raw) / 2 - 2)):\n",
    "        words_inv.append((int(clean_string(word_inv_raw[ind])), clean_string(word_inv_raw[ind + 1])))\n",
    "        ind += 2\n",
    "    meta_raw = np.matrix(pd.read_csv('authors.txt', delimiter='\\t', header=None))\n",
    "    meta = []\n",
    "    au_dir = {}\n",
    "    au_dir_i = 0\n",
    "    for i in meta_raw:\n",
    "        cat_list = i[0, 0].replace(' {', '%[')\n",
    "        full = cat_list.rsplit('%', 1)\n",
    "        cats = {}\n",
    "        cat_list = full[1].translate(str.maketrans('', '', '[]{}\\'\\\":,'))\n",
    "        cat_list = cat_list.split()\n",
    "        ind = 0\n",
    "        for i in range(int(len(cat_list) / 2)):\n",
    "            cats[cat_list[ind]] = float(cat_list[ind + 1])\n",
    "            ind += 2\n",
    "        name_nr = str(full[0]).rsplit(' ', 1)\n",
    "        name = name_nr[0]\n",
    "        au_dir[name.replace(' ','')] = au_dir_i\n",
    "        nr = int(name_nr[1])\n",
    "        meta.append((name, nr, cats))\n",
    "        au_dir_i += 1\n",
    "    cum = np.zeros(len(meta)+1)\n",
    "    tot = 0\n",
    "    for i in range(len(cum)-1):\n",
    "        tot += int(meta[i][1])\n",
    "        cum[i+1] = tot\n",
    "    cum = cum.astype(int)\n",
    "    lam = read('lam_raw.txt')\n",
    "    lam = make_proper(lam, K)\n",
    "    theta = lam.copy()\n",
    "    lam = np.transpose(lam)\n",
    "    lam = scale(lam)\n",
    "    lam = np.transpose(lam)\n",
    "\n",
    "    for i in range(len(theta)):\n",
    "        theta[i] = np.exp(theta[i])/sum(np.exp(theta[i]))\n",
    "    auth_embedding = []\n",
    "    hom = []\n",
    "    for j in range(len(cum)-1):\n",
    "        div = np.full((1, K), (cum[j + 1] - cum[j]))[0]\n",
    "        au_ma = theta[cum[j]:cum[j + 1]]\n",
    "\n",
    "        aggregate = []\n",
    "        homogeneity = []\n",
    "        length = len(au_ma[:, 0])\n",
    "        for k in range(K):\n",
    "            #print(\"var\", np.var(au_ma[:, k]))\n",
    "            so = np.mean((au_ma[:, k]))\n",
    "            aggregate.append(so)\n",
    "            homogeneity.append(np.std(au_ma[:, k]))\n",
    "        div = np.full((1, K), sum(aggregate))[0]\n",
    "        auth_embedding.append(np.array(aggregate)/div)\n",
    "\n",
    "        hom.append(homogeneity)\n",
    "    beta = read('beta_numbers.txt')\n",
    "    beta = make_proper(beta, len(words_inv))\n",
    "\n",
    "    topic_words = []\n",
    "    for j in range(K):\n",
    "        s = list(beta[j])\n",
    "        s_order = []\n",
    "        for i in range(15):\n",
    "            rounded = round(max(s) * 1000) / 1000\n",
    "            s_order.append((words_inv[s.index(max(s))][1]))\n",
    "            s[s.index(max(s))] = -1000\n",
    "        \n",
    "        topic_words.append(s_order)\n",
    "\n",
    "    return meta, theta, beta, words_inv, lam, auth_embedding, topic_words, au_dir, hom\n",
    "\n",
    "meta, theta, beta, words_inv, lam, auth_embedding, topic_words, au_dir, hom = interpret()\n",
    "\n",
    "# Saving of Author Embeddings\n",
    "np.savetxt(\"auth_embed.txt\", auth_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23984e0",
   "metadata": {},
   "source": [
    "\n",
    "## Creation of 'Dash'-suitable graph and functions for graph-functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6f1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx_to_plotly(G, node_cols):\n",
    "    pos = nx.spring_layout(G)\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    text = []\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "        text.append(node)\n",
    "\n",
    "    node_trace = go.Scatter(\n",
    "        x=node_x, y=node_y, text=text,\n",
    "        mode='markers+text',\n",
    "        showlegend=False,\n",
    "        hoverinfo='text',\n",
    "        marker=dict(color=node_cols,\n",
    "                    size=50,\n",
    "                    line=dict(color='white', width=0)))\n",
    "\n",
    "    hover_text = []\n",
    "    for i in range(len(G.nodes())):\n",
    "        k_w_html = ''\n",
    "        for k_word in topic_words[i]:\n",
    "            k_w_html += str(k_word)+'<br>'\n",
    "        hover_text.append(k_w_html)\n",
    "\n",
    "    node_trace['hovertext'] = hover_text\n",
    "\n",
    "    layout = dict(plot_bgcolor='rgb(225,225,245)', #'rgba(18,60,105, 0.3)',\n",
    "                  paper_bgcolor='rgba(18,60,105, 0.0)',\n",
    "                  margin=dict(t=0, b=0,l=0,r=0, pad=0),\n",
    "                  xaxis=dict(linecolor='black',\n",
    "                             showgrid=False,\n",
    "                             showticklabels=False,\n",
    "                             mirror=False,\n",
    "                             visible=False),\n",
    "                  yaxis = dict(linecolor='black',\n",
    "                    showgrid=False,\n",
    "                    showticklabels=False,\n",
    "                    mirror=False,\n",
    "                    visible=False))\n",
    "    \n",
    "    fig = go.Figure(layout=layout)\n",
    "\n",
    "    ex_tuples = []\n",
    "    ey_tuples = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        ex = []\n",
    "        ey = []\n",
    "        ex.append(x0)\n",
    "        ex.append(x1)\n",
    "        ex.append(None)\n",
    "        ey.append(y0)\n",
    "        ey.append(y1)\n",
    "        ey.append(None)\n",
    "        ex_tuples.append(ex)\n",
    "        ey_tuples.append(ey)\n",
    "    line_cols = ['rgb(112,128,144)' for i in range(len(ex_tuples))]\n",
    "\n",
    "    for idx in range(len(ex_tuples)):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=ex_tuples[idx],\n",
    "            y=ey_tuples[idx],\n",
    "            line=dict(\n",
    "                width=2,\n",
    "                color=line_cols[idx],\n",
    "            ),\n",
    "            hoverinfo='none',\n",
    "            showlegend=False,\n",
    "            mode='lines'\n",
    "        ))\n",
    "\n",
    "    fig.add_traces(node_trace)\n",
    "\n",
    "    return fig, node_trace, layout, ex_tuples, ey_tuples, pos\n",
    "\n",
    "# Dictionary of edges for each topic-node to be able to individually address them\n",
    "def K_edge_dict(pos, ex_tuples):\n",
    "    k_edge_dict = {}\n",
    "    #ind = 0\n",
    "    for node in G.nodes():\n",
    "        target = pos[node][0]\n",
    "        target_l = len(ex_tuples)\n",
    "        trace_set = []\n",
    "        for ind in range(target_l):\n",
    "            if ex_tuples[int(ind)][0] == target or ex_tuples[int(ind)][1] == target:\n",
    "                trace_set.append(int(ind))\n",
    "            #ind += 1\n",
    "        k_edge_dict[node] = trace_set[1:]\n",
    "    return k_edge_dict\n",
    "\n",
    "# Update edge weight/opacity according to query\n",
    "def update_edges(node_trace, layout, ex_lines, ey_lines, line_cols):\n",
    "    fig = go.Figure(layout=layout)\n",
    "    if not isinstance(line_cols, str):\n",
    "        for ind, node in enumerate(line_cols):\n",
    "            if node >= 0.1:\n",
    "                for idx in k_edge_dict[ind]:\n",
    "                    fig.add_trace(go.Scatter(\n",
    "                        x=ex_lines[idx],\n",
    "                        y=ey_lines[idx],\n",
    "                        line=dict(\n",
    "                            width=2,\n",
    "                            color='rgba(112,128,144, 0.7)',\n",
    "                        ),\n",
    "                        hoverinfo='none',\n",
    "                        showlegend=False,\n",
    "                        mode='lines'\n",
    "                    ))\n",
    "        fig.add_traces(node_trace)\n",
    "    else:\n",
    "        for idx in range(len(ex_lines)):\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=ex_lines[idx],\n",
    "                y=ey_lines[idx],\n",
    "                line=dict(\n",
    "                    width=2,\n",
    "                    color='rgb(112,128,144)',\n",
    "                ),\n",
    "                hoverinfo='none',\n",
    "                showlegend=False,\n",
    "                mode='lines'\n",
    "            ))\n",
    "        fig.add_traces(node_trace)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75810f",
   "metadata": {},
   "source": [
    "\n",
    "## Filling-level metric and Hellinger-Distance computation\n",
    "FL-metric computation (not used in the actual browser, but used for the thesis and somehow remained in this script) and neighbourliness computation via Hellinger-Distance for selected target author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358987c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket/Filling-level metrics as described in the main work\n",
    "def bucket(a,b):\n",
    "\n",
    "    level = 0\n",
    "    for i in b:\n",
    "        if i in a:\n",
    "            if b[i] > a[i]:\n",
    "                level += a[i]\n",
    "            else:\n",
    "                level += b[i]\n",
    "    return level\n",
    "\n",
    "\n",
    "def bucket_broad(a, b):\n",
    "    a_broad = {}\n",
    "    for i in a:\n",
    "        if (i.split('.'))[0] in a_broad:\n",
    "            a_broad[(i.split('.'))[0]] += a[i]\n",
    "        else:\n",
    "            a_broad[(i.split('.'))[0]] = a[i]\n",
    "    b_broad = {}\n",
    "    for i in b:\n",
    "        if (i.split('.'))[0] in b_broad:\n",
    "            b_broad[(i.split('.'))[0]] += b[i]\n",
    "        else:\n",
    "            b_broad[(i.split('.'))[0]] = b[i]\n",
    "    level = 0\n",
    "    for i in b_broad:\n",
    "        if i in a_broad:\n",
    "            if b_broad[i] > a_broad[i]:\n",
    "                level += a_broad[i]\n",
    "            else:\n",
    "                level += b_broad[i]\n",
    "    return level\n",
    "\n",
    "\n",
    "def ndcg(x, base):\n",
    "    x = x[1:]\n",
    "    x_ideal = np.sort(x)\n",
    "    x_ideal = x_ideal[::-1]\n",
    "    if x_ideal[0] != 0:\n",
    "        dcg_normaliser = 0\n",
    "        dcg = 0\n",
    "        for i, j in enumerate(x_ideal):\n",
    "            dcg_normaliser += j/(np.log(i+2)/np.log(base))\n",
    "        for i, j in enumerate(x):\n",
    "            dcg += j/(np.log(i+2)/np.log(base))\n",
    "\n",
    "        return dcg/dcg_normaliser\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "last_dist = []\n",
    "def dist(a,b, min_filling, inv, bucket_size):\n",
    "    all_lvl = []\n",
    "    distTo = b[a]\n",
    "    a1 = meta[a][2]\n",
    "    neighbours = []\n",
    "    dist = []\n",
    "    m = meta.copy()\n",
    "    hom_i = 0\n",
    "    for o in b:  # Hellinger Distance for discrete probability distributions\n",
    "        d = (1 / (2 ** 0.5)) * (sum(  ((distTo ** 0.5 - o ** 0.5) ** 2))) ** 0.5\n",
    "        #d = sum(distTo*np.log(distTo/o)+o*np.log(o/distTo))\n",
    "        # d = round(d*1000)/1000\n",
    "        dist.append(d)\n",
    "        hom_i += 1\n",
    "    mi_old = -1\n",
    "    #lvl_list = np.zeros(len(meta)-1)\n",
    "    lvl_list = []\n",
    "    if inv == True:\n",
    "        mima = np.max\n",
    "    else:\n",
    "        mima = np.min\n",
    "    if bucket_size == 'broad':\n",
    "        buckets = bucket_broad\n",
    "    elif bucket_size == 'narrow':\n",
    "        buckets = bucket\n",
    "    else:\n",
    "        buckets = 'none'\n",
    "    #print(dist)\n",
    "    for i in range(len(meta)-1):\n",
    "        mi = mima(dist)\n",
    "        ind = dist.index(mi)\n",
    "        if not buckets == 'none':\n",
    "            lvl = buckets(a1, m[ind][2])\n",
    "            lvl = round(1000*lvl)/1000\n",
    "            #neighbours =((m[ind][0], mi, lvl, m[ind][2]))\n",
    "            #lvl_list[i] = float(lvl)\n",
    "            lvl_list.append(float(lvl))\n",
    "            if lvl < min_filling:\n",
    "                if len(neighbours)-1 > 0:\n",
    "                    #print(len(neighbours)-1, ' author(s) found within proximity ', min_filling)\n",
    "                    last_dist.append(mi_old)\n",
    "                    all_lvl.append(np.mean(lvl_list))\n",
    "\n",
    "                break\n",
    "\n",
    "        neighbours.append((m[ind][0], (np.round(1000*mi)/1000))) #, lvl, m[ind][2]))\n",
    "        mi_old = mi\n",
    "        dist.remove(mi)\n",
    "        del m[ind]\n",
    "    all_lvl.append(lvl_list) # ndcg(lvl_list, 2)))\n",
    "    ndcg_ = -1\n",
    "    if not buckets == 'none':\n",
    "        ndcg_ = ndcg(all_lvl[0], 2)\n",
    "    #print('\\n')\n",
    "    #print(neighbours)\n",
    "    #print(all_lvl)\n",
    "    #print(lvl_list)\n",
    "\n",
    "    return lvl_list, neighbours, ndcg_\n",
    "\n",
    "\n",
    "ndcg_tot = 0\n",
    "total = [] # [32, 66, 1, 31, 143, 123, 10, 34, 193, 222]\n",
    "for i in range(len(auth_embedding)):\n",
    "    rand = np.floor(238*random.uniform(0,1))\n",
    "    a, b, c = dist(int(rand), auth_embedding, 0, False, 'broad')\n",
    "    #print(sum(auth_embedding[i]))\n",
    "    total.append(np.array(a))\n",
    "    #print(rand, b[0:2], b[int(100+np.floor(100*random.uniform(0,1)))])\n",
    "    ndcg_tot += c\n",
    "ndcg_tot = ndcg_tot/len(auth_embedding)\n",
    "#print(ndcg_tot)\n",
    "#print(re.sub(r' +', ',', str(sum(total))))\n",
    "\n",
    "def node_config(input, color):\n",
    "    heat_map = []\n",
    "    border_map = []\n",
    "    r, g, b = 254, 156, 143\n",
    "    if color == 'green':\n",
    "        r, g, b = 76, 154, 42\n",
    "    for k in range(len(input)):\n",
    "        col = (r, g, b, input[k])\n",
    "        heat_map.append('rgba'+str(col))\n",
    "        if color == 'green' and input[k] >= 0.4:\n",
    "            border_map.append(3)\n",
    "        elif color == 'green' and input[k] >= 0.2:\n",
    "            border_map.append(1)\n",
    "        else:\n",
    "            border_map.append(0)\n",
    "        #size_map.append((300*input[k]*2))\n",
    "    return heat_map, border_map #, size_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1a30fd",
   "metadata": {},
   "source": [
    "\n",
    "## Compute relevance of authors given query\n",
    "Computes discriminative ranking of all author in dataset given  either a word-query, a selection of topic-nodes or a target-author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57457ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(input):\n",
    "    words = {}\n",
    "    for i in range(len(input)):\n",
    "        words[input[i][1]] = i\n",
    "    return words\n",
    "\n",
    "\n",
    "words = flip(words_inv)\n",
    "\n",
    "\n",
    "def au_recom(str_input):\n",
    "    if str_input == '':\n",
    "        heat_map_and_authors = 'rgba(254,156,143, 1)', '', 'no'\n",
    "    elif str_input.replace(' ','') in au_dir:\n",
    "        str_input = str_input.replace(' ','')\n",
    "        author_i = auth_embedding[int(au_dir[str_input])]\n",
    "        author_i_heat = []\n",
    "        for au_k in author_i:\n",
    "            au_k_heat = 0.01 + au_k/np.max(author_i)\n",
    "            if au_k_heat <= 1:\n",
    "                author_i_heat.append(au_k_heat)\n",
    "            else:\n",
    "                author_i_heat.append(1)\n",
    "        a, b, c = dist(int(au_dir[str_input]), auth_embedding, 0, False, 'none')\n",
    "        author_recom_author = []\n",
    "        for bs in b: # .translate(str.maketrans('', '', '.'))    .replace(' ', '+')\n",
    "            str_number = str(bs[1])\n",
    "            while len(str_number) < 5:\n",
    "                str_number = str_number+'0'\n",
    "            author_recom_author.append(html.A(str_number+' '+str(bs[0]),\n",
    "                                href='https://arxiv.org/search/?query=%22' +\n",
    "                                      ' '+bs[0]+'%22&searchtype=author&source=header',\n",
    "                                 style={'font-family':'Arial, Helvetica, sans-serif', 'font-weight': 'bold', 'color': 'black','text-decoration': 'none'}, target='_blank'))\n",
    "            author_recom_author.append(html.Br())\n",
    "        heat_map_and_authors = author_i_heat, author_recom_author, 'green'\n",
    "    else:\n",
    "        query = clean_string(str_input)\n",
    "        query = query.lower()\n",
    "        query = query.split()\n",
    "        query_lkhood = []\n",
    "        author_lk = []\n",
    "        author_words = []\n",
    "        for k in range(K):\n",
    "\n",
    "            lk = 0\n",
    "            for q in query:\n",
    "                try:\n",
    "                    lk += beta[k, words[q.lower()]]\n",
    "                except:\n",
    "                    lk += 0\n",
    "            if lk == 0:\n",
    "                heat_map_and_authors = 'rgba(254,156,143, 1)', '', 'no'\n",
    "                return heat_map_and_authors\n",
    "\n",
    "            query_lkhood.append(lk)\n",
    "        query_max = np.max(query_lkhood)\n",
    "\n",
    "        for a in auth_embedding:\n",
    "            if not a[0] == 100:\n",
    "            #author_lk.append(np.dot((np.dot(np.diag(a), query_lkhood)), np.ones(K)))\n",
    "            # (1/(2**0.5))*(sum((query_lkhood**0.5-a**0.5)**2))**0.5\n",
    "                author_lk.append((1/(2**0.5))*( sum( (np.array(query_lkhood)**0.5-a**0.5)**2)**0.5 ) )\n",
    "\n",
    "            else:\n",
    "                author_lk.append(100)\n",
    "        author_lk_sorted_names = []\n",
    "        meta_copy = meta.copy()\n",
    "        for i in range(len(meta)):\n",
    "            #ma = np.max(author_lk)\n",
    "            ma = np.min(author_lk)\n",
    "            ma_ind = author_lk.index(ma)\n",
    "            #author_lk_sorted_names.append(meta_copy[ma_ind][0])\n",
    "            author_lk_sorted_names.append(html.A(' '+str(meta_copy[ma_ind][0]), href='https://arxiv.org/search/?query=%22'+str((meta_copy[ma_ind][0].translate(str.maketrans('', '', '.'))).replace(' ','+'))+'%22&searchtype=author&source=header', style={'font-family':'Arial, Helvetica, sans-serif', 'font-weight':'bold', 'color':'black','text-decoration':'none'}, target='_blank'))\n",
    "            author_lk_sorted_names.append(html.Br())\n",
    "            author_lk.remove(ma)\n",
    "            del meta_copy[ma_ind]\n",
    "        for q in range(len(query_lkhood)):\n",
    "            query_lkhood[q] = 0.01+query_lkhood[q]/query_max  # round(10000*query_lkhood[q])/10000\n",
    "            if query_lkhood[q] > 1:\n",
    "                query_lkhood[q] = 1\n",
    "        heat_map_and_authors = query_lkhood, author_lk_sorted_names, 'no'\n",
    "    return heat_map_and_authors\n",
    "\n",
    "def k_node_authors(k):\n",
    "    # most likely authors given one specific topic\n",
    "    auth_embedding_k = []\n",
    "    auth_embedding_k_sorted = []\n",
    "    meta_copy = meta.copy()\n",
    "    for a in auth_embedding:\n",
    "        sub = [a[i] for i in k]\n",
    "        auth_embedding_k.append(sum(sub))\n",
    "    for ak in range(len(meta)):\n",
    "        m = np.max(auth_embedding_k)\n",
    "        m_ind = auth_embedding_k.index(m)\n",
    "        auth_embedding_k_sorted.append(html.A(' '+str(meta_copy[m_ind][0]), href='https://arxiv.org/search/?query=%22'+str((meta_copy[m_ind][0].translate(str.maketrans('', '', '.'))).replace(' ','+'))+'%22&searchtype=author&source=header', style={'font-family':'Arial, Helvetica, sans-serif', 'font-weight':'bold', 'color':'black','text-decoration':'none'}, target='_blank'))\n",
    "        auth_embedding_k_sorted.append(html.Br())\n",
    "        auth_embedding_k.remove(m)\n",
    "        del meta_copy[m_ind]\n",
    "\n",
    "    return auth_embedding_k_sorted\n",
    "\n",
    "# Globals\n",
    "G = graph(lam, K, 0.2)\n",
    "G_drawn, node_trace, layout, ex, ey, pos = nx_to_plotly(G, 'rgba(254,156,143, 1)')\n",
    "k_edge_dict = K_edge_dict(pos, ex)\n",
    "container = []\n",
    "container_cols = ['rgba(254,156,143, 0.2)' for i in range(K)]\n",
    "ret_store = G_drawn, [], html.H3('Recommendations for Query'), False\n",
    "topicSearch = False\n",
    "#post_activation = False\n",
    "post_activation_clicks = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7f67a",
   "metadata": {},
   "source": [
    "\n",
    "## Dash Layout\n",
    "HTML in python made possible with Dash Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54f0d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = dash.Dash(__name__)\n",
    "app.title = 'Visual Topic Model'\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.Div([dcc.Graph(id='G_network', figure=G_drawn),\n",
    "              dcc.ConfirmDialog(id='no_match', message='No match found'),\n",
    "              ]),\n",
    "    html.Div(\n",
    "        className=\"slider_container\", style={'font-family':'Arial, Helvetica, sans-serif', 'font-weight': 'bold'},\n",
    "        children=[\n",
    "            dcc.Slider(\n",
    "                id='alpha_slider',\n",
    "                min=0.05,\n",
    "                max=1,\n",
    "                step=0.05,\n",
    "                value=0.2\n",
    "\n",
    "            ),\n",
    "            html.Div(id='alpha_slider_output')\n",
    "        ]\n",
    "    ),\n",
    "    html.Div([   html.Button('Reset', id='button_search', n_clicks=0, hidden=False, style={'background-color':'rgba(18,60,105, 0.2)'}),\n",
    "                dcc.Input(id='searchbar', type='text', value='', placeholder='enter query', debounce=True,\n",
    "                style={'width': '70%'}),\n",
    "                 html.Button('Topic Search', value=\"one\", id='topical_search', n_clicks=0, hidden=False, style={'background-color':'rgba(18,60,105, 0.2)'})], id=\"div_searchbar\", style={'text-align':'center', 'margin-top':'2%','margin-bottom':'2%', 'margin': 'auto'}),  # , style=dict(display='none'))\n",
    "\n",
    "\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.Div(id='left-recommender-column'),\n",
    "            html.Div(id='cum_LK_author_for_query')\n",
    "            ], className='six columns'),\n",
    "        html.Div([\n",
    "            html.Div(id='author-topic-node'),\n",
    "            html.Div(id='recommendations')\n",
    "            ], className='six columns')\n",
    "      ], className='row')\n",
    "\n",
    "], style={'background-color':'rgba(225,225,245, 1)'}) #18,60,105, 0.3\n",
    "\n",
    "app.scripts.append_script({\n",
    "    'external_url': '/assets/style.css'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cdf751",
   "metadata": {},
   "source": [
    "## Dash functions\n",
    "Dash functions providing actual interactivity with inferface and graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2277d335",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.callback(\n",
    "    Output(component_id='G_network', component_property='figure'),\n",
    "    Output(component_id='cum_LK_author_for_query',component_property='children'),\n",
    "    Output(component_id='left-recommender-column', component_property='children'),\n",
    "    Output(component_id='no_match', component_property='displayed'),\n",
    "    Input(component_id='searchbar', component_property='value'),\n",
    "    Input(component_id='alpha_slider', component_property='value'),\n",
    "    Input(component_id='G_network', component_property='clickData'),\n",
    "    Input(component_id='topical_search', component_property='n_clicks'),\n",
    "    state=[State('topical_search', 'value')],\n",
    "    )\n",
    "def grapher(query, a, clickData, n_clicks, value):\n",
    "    global topicSearch, G_drawn, container, container_cols, ret_store, post_activation_clicks\n",
    "    global G_drawn, node_trace, layout, ex, ey, pos, k_edge_dict\n",
    "    ctx = dash.callback_context\n",
    "    button_diffferentiatior = ctx.triggered[0]['prop_id'].split('.')[0]\n",
    "    #print(button_diffferentiatior)\n",
    "    h3 = html.H3('Recommendations for Query')\n",
    "    ret = G_drawn, [], h3, False\n",
    "    if ctx.triggered[0]['prop_id'] == 'searchbar.value':\n",
    "        #print(\"0\")\n",
    "        q_res, author_query_LK, green = au_recom(query)\n",
    "        col, border = node_config(q_res, green)\n",
    "\n",
    "        if q_res == 'rgba(254,156,143, 1)' and green=='no' and query is not \"\":\n",
    "            #print(\"2\")\n",
    "            G_drawn.update_traces(marker=dict(color=q_res, line=dict(width=0)))\n",
    "            ret = G_drawn, [], h3, True\n",
    "\n",
    "        elif q_res == 'rgba(254,156,143, 1)':\n",
    "            #print(\"3\")\n",
    "            #global topicSearch\n",
    "            topicSearch = False\n",
    "            post_activation_clicks = 0\n",
    "            container = []\n",
    "            container_cols = ['rgba(254,156,143, 0.2)' for i in range(K)]\n",
    "            G_drawn = update_edges(node_trace, layout, ex, ey, 'rgb(112,128,144)')\n",
    "            G_drawn.update_traces(marker=dict(color='rgba(254,156,143, 1)', line=dict(width=0)))\n",
    "            ret_store = G_drawn, [], html.H3('Recommendations for Query'), False\n",
    "            ret = G_drawn, [], h3, False\n",
    "\n",
    "        else:\n",
    "            #print(\"4\")\n",
    "            G_drawn = update_edges(node_trace, layout, ex, ey, q_res)\n",
    "            G_drawn.update_traces(marker=dict(color=col, line=dict(width=border)))\n",
    "            if green == 'green':\n",
    "                ret_store = G_drawn, author_query_LK, html.H3('Recommendations for Author'), False\n",
    "            else:\n",
    "                ret_store = G_drawn, author_query_LK, h3, False\n",
    "            ret = ret_store\n",
    "\n",
    "    elif (button_diffferentiatior == 'G_network' and topicSearch is True):\n",
    "        #print(\"5\")\n",
    "        node_k = str(clickData).replace(\"text': '\", '_x_x_')\n",
    "        node_k = (node_k.split('_x_x_'))[1]\n",
    "        node_k = (node_k.split(\"'\", 1))[0]\n",
    "        if int(node_k) in container:\n",
    "            container.remove(int(node_k))\n",
    "            container_cols[int(node_k)] = 'rgba(254,156,143, 0.2)'\n",
    "            G_drawn.update_traces(marker=dict(color=container_cols, line=dict(width=0)))\n",
    "        else:\n",
    "            container.append(int(node_k))\n",
    "            container_cols[int(node_k)] = 'rgba(254,156,143, 1)'\n",
    "            G_drawn.update_traces(marker=dict(color=container_cols, line=dict(width=0)))\n",
    "        ret = G_drawn, [], h3, False\n",
    "    elif button_diffferentiatior == \"topical_search\":\n",
    "        topicSearch = True\n",
    "        #print(\"6\")\n",
    "        if post_activation_clicks == 0:\n",
    "            post_activation_clicks = 1\n",
    "            G_drawn = update_edges(node_trace, layout, ex, ey, 'rgb(112,128,144)')\n",
    "            G_drawn.update_traces(marker=dict(color=\"rgba(254,156,143, 0.2)\", line=dict(width=0)))\n",
    "            ret = G_drawn, [], h3, False\n",
    "    elif button_diffferentiatior=='alpha_slider':\n",
    "        #print(\"8\")\n",
    "        G_drawn, node_trace, layout, ex, ey, pos = nx_to_plotly(graph(lam, K, a), 'rgba(254,156,143, 1)')\n",
    "        k_edge_dict = K_edge_dict(pos, ex)\n",
    "        ret = G_drawn, [], h3, False\n",
    "    else:\n",
    "        #print(\"7\")\n",
    "        ret = ret_store\n",
    "    return ret\n",
    "\n",
    "@app.callback(\n",
    "    Output(component_id='alpha_slider_output',component_property='children'),\n",
    "    Input(component_id='alpha_slider',component_property='value')\n",
    ")\n",
    "def selected_alpha(a):\n",
    "    return 'Sparsity: '+str(a)\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('recommendations', 'children'),\n",
    "    Output('author-topic-node', 'children'),\n",
    "    Input('G_network', 'clickData'),\n",
    "    Input(component_id='button_search', component_property='n_clicks')\n",
    ")\n",
    "def on_click(clickData, n_clicks):\n",
    "    ctx = dash.callback_context\n",
    "    button_diffferentiatior = ctx.triggered[0]['prop_id'].split('.')[0]\n",
    "    ret = [], html.H3('Most likely authors for topic node none')\n",
    "    time.sleep(0.25)\n",
    "    if button_diffferentiatior == 'G_network':\n",
    "        node_k = str(clickData).replace(\"text': '\", '_x_x_')\n",
    "        #print(node_k)\n",
    "        node_k = (node_k.split('_x_x_'))[1]\n",
    "        node_k = (node_k.split(\"'\", 1))[0]\n",
    "\n",
    "        if topicSearch is False:\n",
    "            res = k_node_authors([int(node_k)])\n",
    "            h3 = html.H3('Most likely authors for topic node '+str(int(node_k)))\n",
    "            ret = res, h3\n",
    "        elif post_activation_clicks == 1:\n",
    "            if len(container) > 0:\n",
    "                res = k_node_authors(container)\n",
    "            else:\n",
    "                res = []\n",
    "            h3 = html.H3('Most likely authors for topic search')\n",
    "            ret = res, h3\n",
    "    elif ctx.triggered[0]['prop_id'] == 'button_search.n_clicks':\n",
    "        ret = [], html.H3('Most likely authors for topic node none')\n",
    "\n",
    "\n",
    "    return ret\n",
    "\n",
    "@app.callback(\n",
    "    Output(component_id='searchbar', component_property='value'),\n",
    "    Input(component_id='button_search', component_property='n_clicks'),\n",
    "    state=[State('button_search', 'value')]\n",
    ")\n",
    "def b(n_clicks, value):\n",
    "    if n_clicks == 0:\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26473ad",
   "metadata": {},
   "source": [
    "\n",
    "#### How to run above script on your own\n",
    "The above script uses the Dash library. Unfortunately, Jupyter and Dash do not communicate easily with one another.\n",
    "A working version of the above can be found here (http://ivoarasin.pythonanywhere.com/). Additionally, all data files are provided along with the actual python script for the interactive browser, so setting it up for yourself should not be too hard."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
